{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0xP8UX0W5KDqIJBShbdeY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Liang130520/Tibame-Machine-Learning/blob/main/class1102_machine_learning_poems_7_%E9%AB%98%E6%96%AF%E5%96%AE%E7%B4%94%E8%B2%9D%E6%B0%8F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 944
        },
        "id": "fIsut7MVz8nR",
        "outputId": "bc1e6924-8afc-4af4-b45b-9c7d32cf1466"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 21652)\t1\n",
            "  (0, 29325)\t1\n",
            "  (0, 32404)\t1\n",
            "  (0, 33259)\t1\n",
            "  (0, 36144)\t1\n",
            "  (0, 40411)\t1\n",
            "  (0, 45388)\t1\n",
            "  (0, 46901)\t1\n",
            "  (0, 50588)\t1\n",
            "  (1, 201)\t1\n",
            "  (1, 1037)\t1\n",
            "  (1, 5321)\t1\n",
            "  (1, 7398)\t1\n",
            "  (1, 15739)\t1\n",
            "  (1, 23671)\t1\n",
            "  (1, 26945)\t1\n",
            "  (1, 30993)\t1\n",
            "  (1, 32734)\t1\n",
            "  (1, 44300)\t1\n",
            "  (1, 46477)\t1\n",
            "  (2, 1092)\t1\n",
            "  (2, 2160)\t1\n",
            "  (2, 7343)\t1\n",
            "  (2, 15312)\t1\n",
            "  (2, 17635)\t1\n",
            "  :\t:\n",
            "  (26, 18010)\t1\n",
            "  (26, 19510)\t1\n",
            "  (26, 19833)\t1\n",
            "  (26, 22302)\t1\n",
            "  (26, 23398)\t1\n",
            "  (26, 24210)\t1\n",
            "  (26, 29775)\t1\n",
            "  (26, 51930)\t1\n",
            "  (27, 16)\t1\n",
            "  (27, 1131)\t1\n",
            "  (27, 1361)\t1\n",
            "  (27, 3576)\t1\n",
            "  (27, 3638)\t1\n",
            "  (27, 5425)\t1\n",
            "  (27, 10217)\t1\n",
            "  (27, 12065)\t1\n",
            "  (27, 14735)\t1\n",
            "  (27, 21864)\t1\n",
            "  (27, 22290)\t1\n",
            "  (27, 22338)\t1\n",
            "  (27, 47253)\t1\n",
            "  (28, 415)\t1\n",
            "  (28, 21864)\t1\n",
            "  (28, 47455)\t1\n",
            "  (29, 50837)\t1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'=============================================================================='"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from urllib.request import urlretrieve\n",
        "url = \"https://github.com/Elwing-Chou/tibaml1017/raw/main/poem_train.csv\"\n",
        "urlretrieve(url, \"train.csv\")\n",
        "url = \"https://github.com/Elwing-Chou/tibaml1017/raw/main/poem_test.csv\"\n",
        "urlretrieve(url, \"test.csv\")\n",
        "\n",
        "\"==============================================================================\"\n",
        "import pandas as pd\n",
        "train_df = pd.read_csv(\"train.csv\", encoding = \"utf-8\")\n",
        "test_df = pd.read_csv(\"test.csv\", encoding = \"utf-8\")\n",
        "\n",
        "\n",
        "test_df\n",
        "\n",
        "\"==============================================================================\"\n",
        "# 進到所有函式庫之前, 不得為series, 需全部為數字\n",
        "# pandas 有兩種形式, 有顏色差異的是二維的 data frame, 沒顏色差異的是一維 series [test_df是data frame, ]\n",
        "# Series.value_counts/unique：value_counts會幫忙計算個別與總體數量, unique只會顯示有哪幾種不同資料\n",
        "# poets = train_df[\"作者\"].value_counts()\n",
        "# poets\n",
        "\n",
        "\" 將作者修改成數字 方法1 \"\n",
        "# Series.replace(字典) [\"李白\":0, \"杜甫\":1, \"白居易\":2]\n",
        "\n",
        "\" 將作者修改成數字 方法2 \"\n",
        "poets = train_df[\"作者\"].unique()\n",
        "# poet2idx = []\n",
        "# for i in range(len(poets)):\n",
        "#   poet2idx[poets[i]] = i\n",
        "poet2idx = {poets[i]:i for i in range(len(poets))} # 這行等於上面3行, python的特殊寫法\n",
        "idx2poet = {i:poets[i] for i in range(len(poets))} # 將 index 再轉回作者名\n",
        "poet2idx\n",
        "\n",
        "\"==============================================================================\"\n",
        "import numpy as np\n",
        "y_train = np.array(train_df[\"作者\"].replace(poet2idx))\n",
        "y_test = np.array(test_df[\"作者\"].replace(poet2idx))\n",
        "y_test\n",
        "\n",
        "\"==============================================================================\"\n",
        "\" Every thing is an object \" # python 內每樣東西都有其形態, 都有其操作樣式\n",
        "# 型態 + 操作\n",
        "# list + [idx]\n",
        "# dict + [key]\n",
        "# print(\"hello\") --> print:(步驟)型態, (\"hello\"):(執行)操作\n",
        "b = abs # 沒加(), 是為型態, abs是絕對值型態\n",
        "b(-5.23)\n",
        "def test(n):\n",
        "  if n == 0:\n",
        "    return np.array\n",
        "  elif n == 1:\n",
        "    return pd.Series\n",
        "  else:\n",
        "    return list\n",
        "test(2)([1, 2, 3, 4]) # test(n) -> 未打開的錦囊, ([1, 2, 3, 4]) -> 打開錦囊要的操作\n",
        "\n",
        "\"==============================================================================\"\n",
        "import jieba\n",
        "# # 通常使用jieba, 必定載入dict, (但因為古文與現代文有差異, 且老師試過, 此處不適合載入大辭典)\n",
        "# s = \"平林默默煙如織, 寒山一帶傷心碧\"\n",
        "# \" \".join(jieba.cut(s))\n",
        "# # train_df[\"內容\"]\n",
        "# Series.apply(func)\n",
        "def trans(s): \n",
        "  return \" \".join(jieba.cut(s))\n",
        "x_train = train_df[\"內容\"].apply(trans) # 尚未需要執行, 無須在trans後加()\n",
        "x_test = test_df[\"內容\"].apply(trans) # apply 是打算處理成類似英文, 空格差異, 逐個當作參數傳入\n",
        "x_test\n",
        "\n",
        "\"==============================================================================\"\n",
        "\"\"\"\n",
        "if          詞\"我\"      詞\"你\"      詞\"他\"      依貝氏單純計算(P(李白/我, 你, 牛) = P(李白)*P(我/李白)*P(你/李白)*P(牛/李白))\n",
        "1.李白        2           0           2         2/3 * 5/13 * 3/13 * [因為無資料, 忽略(牛/李白)]\n",
        "2.杜甫        1           1           1\n",
        "3.李白        3           3           3\n",
        "\n",
        "\"\"\"\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# CountVectorizer 是看字詞出現幾遍\n",
        "# fit: 找出幾個欄位\n",
        "# transform: 找出字詞出現幾遍\n",
        "\" 訓練資料 \" # -> fit + transform\n",
        "\" 測試資料 \" # -> transform\n",
        "\n",
        "vec = CountVectorizer()\n",
        "x_train_count = vec.fit_transform(x_train)\n",
        "x_test_count = vec.transform(x_test)\n",
        "\n",
        "\"==============================================================================\"\n",
        "\n",
        "\n",
        "vec.vocabulary_ # 取得內容裡的分詞次數\n",
        "# # 標點符號不影響判斷, 該被去掉, 記得一一檢查\n",
        "# vec.vocabulary_[\",\"] # sklearn 已協助去掉\n",
        "# # 換行符號(/r/n): 同樣該被去掉\n",
        "# vec.vocabulary_[\"/r\"] / vec.vocabulary_[\"/n\"] # sklearn 已協助去掉\n",
        "x_train_count # 先取得 x_train 共有多少詞, 得 2731 首詩 * 52294 種詞, 其中 85677 為非 0 位置\n",
        "# print(x_train_count)\n",
        "# 稀疏矩陣(Sparse Matrix) -> 不儲存 0 的資料, 只存非 0 位置\n",
        "\n",
        "\n",
        "\n",
        "\"==============================================================================\"\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "# 採用 naive_bayes 的演算法\n",
        "clf = MultinomialNB(alpha = 0.25)\n",
        "clf.fit(x_train_count, y_train)\n",
        "\n",
        "print(x_test_count)\n",
        "\n",
        "\"==============================================================================\"\n",
        "from sklearn.metrics import accuracy_score\n",
        "pre = clf.predict(x_test_count)\n",
        "accuracy_score(y_test, pre)\n",
        "\n",
        "\"==============================================================================\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "if          詞\"我\"      詞\"你\"      詞\"他\"      依貝氏單純計算(P(李白/我, 你, 牛) = P(李白)*P(我/李白)*P(你/李白)*P(牛/李白))\n",
        "1.李白        2           0           2         2/3 * 5/13 * 3/13 * [因為無資料, 忽略(牛/李白)]\n",
        "2.杜甫        1           0           1\n",
        "3.李白        3           3           3\n",
        "4.杜甫        0           0           2\n",
        "\n",
        "\"\"\"\n",
        "# P(杜甫/我, 你) = P(杜甫) * P(我/杜甫) * P(你/杜甫) = 2/4 * 1/4 * 0/4\n",
        "# -> 代表只要出現\"你\", 絕對不會是杜甫的資料, 但實際上無法保證未來不出現\n",
        "# 不好的機率為 0% 或 100% \n",
        "\n",
        "\" 避開 0% (Smoothing) \" # 保底次數\n",
        "# theta = (\"我\"次數 + alpha) / (總詞數 + (alpha * 總欄數))\n",
        "# 令 alpha = 1, 則每一面(欄) = 1 開始計數\n",
        "# -> P(杜甫/我, 你) = P(杜甫) * P(我/杜甫) * P(你/杜甫) = 2/4 * (1+1)/(4+3) * (0+1)/(4+3)\n",
        "\n",
        "# alpha 數值依據原數值去判斷給予比率, 若原數值夠大, 則可設定略大(原10000,alpha 1-100影響不大), 若原數值很小, 則須設定略小(原10, alpha 1仍偏大, 但只能慢慢試)\n",
        "# alpha 可為浮點數\n",
        "\n",
        "# 調了上面的 MultinomialNB(alpha = 0.25)"
      ],
      "metadata": {
        "id": "7UnOwE5H7z-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\" 多項式機率 \"\n",
        "# 令骰骰子時, \"我\"出現的次數 -> x1, \"你\"出現的次數 -> x2 [次數不一定為整數, 可為浮點數]\n",
        "# P(杜甫/我, 我, 你) = P(杜甫) * P(我/杜甫) * P(我/杜甫) * P(你/杜甫) = P(杜甫) * P(我/杜甫) ** x1 * P(你/杜甫) ** x2\n",
        "\n",
        "# P1 ** n1 * P2 ** n2 * P3 ** n3 * ..........\n",
        "# -> P1 = (某事件次數 + alpha) / (總事件次數 + (alpha * 總欄數))"
      ],
      "metadata": {
        "id": "g9JT8W-GXf8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\" 維度災難(欄位過多) \" # 解決方案 -> 單純貝氏 1.只考慮出現的欄位機率 2.架設事件獨立, 資料量可大幅減少\n",
        "# if 欄位過多, 導致判斷資料過少, 則很難斷定為有效判定通則, 同樣可視為例外\n",
        "# 欄位過多, 導致事件交互影響, 10 個因素 * 10個因素, 導致需要足夠資料量(恐為指數級成長)才能支持相關性"
      ],
      "metadata": {
        "id": "SbvWX2q0Xmhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\" 高斯單純貝氏 處理鳶尾花資料 \"\n",
        "# if sepal lenth = 2.2, pedal lenth = 4.2, 預測為何種花\n",
        "# 各種花比機率大小\n",
        "# P(Setosa / sl = 2.2, pl = 4.2) = P(Setosa) * P(sl = 2.2 / Setosa) * P(pl = 4.2 / Setosa)\n",
        "# -> 有限的值去推算無限的機率, 以常態分佈去思考, 當我取得「平均值」跟「標準差(和平均值的距離平均)」就能取得常態分佈代表的機率值\n"
      ],
      "metadata": {
        "id": "UE0rnV7zxl_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" 單純貝氏 估算機率\n",
        "1.值:有限種 -> 骰子(poem)\n",
        "2.值:無限種 -> 高斯單純貝氏(iris) [離獨立事件過遠, 所以判斷沒那麼好]\n",
        " \"\"\""
      ],
      "metadata": {
        "id": "bdaq6eEzxprd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}